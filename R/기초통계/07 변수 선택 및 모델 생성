#==================================================
# 7. 모델생성
#==================================================
getwd()
setwd("c://temp//R")

# 7.1 Train Set & Test Set ---------------------------------------------------------
rm(list=ls())
#Sampling Indexes
titanic.0 <- read.csv("titanic.1.csv" 
                      , stringsAsFactors = F 
                      , na.strings=c("","NA","Unknown","NULL"))

titanic.0$Sex <- as.factor(titanic.0$Sex)
titanic.0$Survived <- as.factor(titanic.0$Survived)
titanic.0$Embarked <- as.factor(titanic.0$Embarked)
titanic.0$Pclass <- as.factor(titanic.0$Pclass)
titanic.0$AgeGroup <- as.factor(titanic.0$AgeGroup)

?sample
sample(1:100, size = 10)
nrow(titanic.0)

set.seed(2019)
indexes <- sample(1:nrow(titanic.0)
                  , size=0.3*nrow(titanic.0))

indexes
# Split data
test <- titanic.0[indexes,]
train <- titanic.0[-indexes,]
nrow(test)
nrow(train)

# 데이터 들여다보기
# 주요 변수의 비율 비교 : titanic.0, train, test
table(titanic.0$Sex)
table(titanic.0$Survived)
prop.table(table(titanic.0$Sex))
prop.table(table(train$Sex))
prop.table(table(test$Sex))

prop.table(table(titanic.0$Survived))
prop.table(table(train$Survived))
prop.table(table(test$Survived))


# 7.2 변수 선택하기 : 전진선택법 ---------------------------------------------------
glm.base <- glm(Survived~1, family=binomial, data = train)
str(train)
forward.aic <- step(glm.base
                    , Survived~Age+Embarked+Fare+Sex+Pclass+SibSp+Parch+Family+AgeGroup
                    ,direction = "forward")

# Survived ~ Sex + Pclass + Age + SibSp  + Embarked
# Survived ~ Sex + Pclass + AgeGroup + SibSp  + Embarked
# 내가선택된 변수 ==>  Sex + Pclass + Age + Embarked + Fare

# 7.3 머신러닝 알고리즘 : 로지스틱회귀 ---------------------------------------------

#모델생성
train.glm.1 <- glm(Survived ~ Sex + Pclass + Age + SibSp  + Embarked
                   , family = binomial
                   , data = train)

train.glm.2 <- glm(Survived ~ Sex + Pclass + AgeGroup + SibSp  + Embarked
                   , family = binomial
                   , data = train)

train.glm.3 <- glm(Survived ~ Sex + Pclass + Age + Embarked + Fare
                   , family = binomial
                   , data = train)
#예측
head(test)
fit.results.1 <- predict(train.glm.1, newdata=test[,-2], type='response')
fit.results.2 <- predict(train.glm.2, newdata=test[,-2], type='response')
fit.results.3 <- predict(train.glm.3, newdata=test[,-2], type='response')
fit.results.1

#==================================================
# 8. 검증
#==================================================

# 8.1 ROC(Receiver Operating Curve) --------------

install.packages("Epi")
require(Epi)

ROC(fit.results.1,test$Survived  ) # 0.404, 0.864
ROC(fit.results.2,test$Survived  ) # 0.389, 0.862
ROC(fit.results.3,test$Survived  ) # 0.415, 0.853

# 8.2 Cut-off value ----
# 0.657
# 0.598
# 1,0으로 바꿔주기 
fit.results.New.1 <- ifelse(fit.results.1 >= 0.404,1,0)
fit.results.New.2 <- ifelse(fit.results.2 >= 0.389,1,0)
fit.results.New.3 <- ifelse(fit.results.3 >= 0.415,1,0)

# 8.3 Confusion matrix -------------------------------
if(!require(caret)) install.packages("caret")
library(caret)
if(!require(e1071)) install.packages("e1071")
library(e1071)

# 수정된 부분 : confusionMatrix(예측값, 실제값)으로 실행
fit.results.New.1 <- as.factor(fit.results.New.1)
fit.results.New.2 <- as.factor(fit.results.New.2)
fit.results.New.3 <- as.factor(fit.results.New.3)

confusionMatrix(fit.results.New.1,test$Survived)
confusionMatrix(fit.results.New.2,test$Survived)
confusionMatrix(fit.results.New.3,test$Survived)

##############################
# 연습문제 : 모델링1
##############################

# 전처리가 끝난 titanic dataset을 이용하여 다음 연습문제를 푸시오.
rm(list=ls())

titanic.0 <- read.csv("titanic.1.csv" 
                      , stringsAsFactors = F 
                      , na.strings=c("","NA","Unknown","NULL"))

titanic.0$Sex <- as.factor(titanic.0$Sex)
titanic.0$Survived <- as.factor(titanic.0$Survived)
titanic.0$Embarked <- as.factor(titanic.0$Embarked)
titanic.0$Pclass <- as.factor(titanic.0$Pclass)
titanic.0$AgeGroup <- as.factor(titanic.0$AgeGroup)

# 1. 데이터셋을 train set과 test set을 8:2로 나누시오
indexes <- sample(1:nrow(titanic.0)
                  , size=0.2*nrow(titanic.0))
indexes
# Split data
test <- titanic.0[indexes,]
train <- titanic.0[-indexes,]


# 2. 전진선택법으로 변수를 선택하시오
glm.base <- glm(Survived ~ 1, family=binomial, data = train)
str(train)
forward.aic <- step(glm.base
                    , Survived~Age+Embarked+Fare+Sex+Pclass+SibSp+Parch+Family+AgeGroup
                    ,direction = "forward")



# 내가 선택한 변수들 : Sex + Pclass + Fare + Embarked
# 전진선택법으로 찾아준 변수들 : Survived ~ Sex + Pclass + AgeGroup + Family

# 3. 로지스틱 회귀분석 모델 네가지를 생성하시오
# 1) EDA,CDA를 통해서 선택된 독립변수들을 이용한 모델1
# 2) 모델1에서 변수를 하나 추가/혹은 제거한 모델2
# 3) 모델1또는 2에서, 강한 Age 대신 Agegroup 혹은 Fare 대신 Fare2로 바꾼 모델3
# 4) 전진선택법으로 선택된 독립변수들을 이용한 모델

train.glm.1 <- glm(Survived ~ Sex + Pclass + Fare + Embarked
                   , family = binomial
                   , data = train)

train.glm.2 <- glm(Survived ~ Sex + Pclass + Fare2 + Embarked
                   , family = binomial
                   , data = train)

train.glm.3 <- glm(Survived ~ Sex + Pclass + AgeGroup + Family
                   , family = binomial
                   , data = train)

train.glm.4 <- glm(Survived ~ Sex + Pclass + Age + Family
                   , family = binomial
                   , data = train)
summary(train.glm.4)


# 4. 3번에서 생성된 모델들로 test set을 이용하여 각각 예측결과를 저장하시오.


str(test)
fit.results.1 <- predict(train.glm.1, newdata=test[,-2], type='response')
fit.results.2 <- predict(train.glm.2, newdata=test[,-2], type='response')
fit.results.3 <- predict(train.glm.3, newdata=test[,-2], type='response')
fit.results.4 <- predict(train.glm.4, newdata=test[,-2], type='response')

##############################
# 연습문제 : 모델링2
##############################

# 연습문제 : 모델링1의 결과에 이어서 진행하시오.
# 1. 모델 4개에 대해서 각각 ROC 커브를 각각 그려보세요.
require(Epi)

ROC(fit.results.1,test$Survived  ) # 0.578, 0.818
ROC(fit.results.2,test$Survived  ) # 0.578, 0.818
ROC(fit.results.3,test$Survived  ) # 0.403, 0.843
ROC(fit.results.4,test$Survived  ) # 0.578, 0.818

# 2. 네 모델의 AUC를 구하여 비교하시오.



# 3. 각각, 최적의 cut-off value를 찾으시오.




# 4. 최적의 cut-off value를 적용하여 모델링1에서 예측 결과를 1, 0로 나누시오.
fit.results.New.1 <- ifelse(fit.results.1 >= 0.578,1,0)
fit.results.New.3 <- ifelse(fit.results.3 >= 0.403,1,0)



# 5. confusionMatrix를 이용하여 성능을 비교하시오.

if(!require(caret)) install.packages("caret")
library(caret)
if(!require(e1071)) install.packages("e1071")
library(e1071)

fit.results.New.1 <- as.factor(fit.results.New.1)
fit.results.New.3 <- as.factor(fit.results.New.3)

confusionMatrix(fit.results.New.1,test$Survived, positive = "1")
confusionMatrix(fit.results.New.3,test$Survived, positive = "1")


# 6. (option) 각 모델의 cut-off value를 앞 뒤로 조정하면서,
# accuracy를 적절히 보장하면서 민감도를 높여 보시오.
roc.3 <- ROC(fit.results.3,test$Survived  )
cutval <- max(roc.3$res[roc.3$res$sens > 0.8,]$fit.results.3)
fit.results.New.31 <- ifelse(fit.results.3 >= cutval,1,0)
fit.results.New.31 <- as.factor(fit.results.New.31)
confusionMatrix(fit.results.New.31,test$Survived, positive = "1")
